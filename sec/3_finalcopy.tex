\section{Limitations and Failed Ideas}

Some approaches did not generalise effectively. The k-NN classifier, intended to complement linear and tree-based models, contributed only marginal improvements. Random Forest models suffered from severe overfitting and performed significantly worse than XGBoost.

\section{Conclusion}

This work shows that competitive Pok√©mon battles can be effectively modelled through a combination of feature engineering and ensemble learning. Logistic Regression and XGBoost provided complementary strengths, and ensemble strategies further improved robustness.
