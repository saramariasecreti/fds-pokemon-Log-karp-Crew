\section{Limitations and Failed Ideas}

Some approaches did not generalise effectively. The k-NN classifier, intended to complement linear and tree-based models, contributed only marginal improvements. Random Forest models suffered from severe overfitting and performed significantly worse than XGBoost. 
During development, occasional implementation errors emerged, mainly due to the complexity of the feature pipeline and model interfaces. In order to perform faster and more precise debugging, we relied on the Claude chatbot for targeted code inspection and error identification.

\section{Conclusion}

This work shows that competitive Pok√©mon battles can be effectively modelled through a combination of feature engineering and ensemble learning. Logistic Regression and XGBoost provided complementary strengths, and ensemble strategies further improved robustness.

